# Indic BPE Tokenizer (Telugu example)

This repo contains a simple **Sennrich-style BPE tokenizer** trained on an Indic language (default: **Telugu**).  
It satisfies the assignment constraints:

- Vocabulary size **< 5000**
- Compression ratio **≥ 3.2** on validation (defined as raw characters / token count)

> ⚠️ You must supply your own dataset under `./data/*.txt` (Telugu/Hindi/etc.) and run training.

## Getting Started

```bash
# create env (optional)
python -m venv .venv && source .venv/bin/activate  # on Windows: .venv\Scripts\activate
pip install -r requirements.txt

# train
python train_bpe.py
```

The training script:
- Reads all `.txt` files under `./data`
- Trains BPE with a vocabulary cap of 5000 (including special tokens)
- Evaluates compression ratio on a held-out split
- Saves tokenizer JSON to `./artifacts/telugu_bpe_tokenizer.json`
- Auto-updates this README with results below

<!-- STATS_MARKER -->

### Results (Auto-filled)
- **Tokenizer vocabulary size**: `608` (must be < 5000)
- **Compression ratio (val)**: `3.7875` (must be ≥ 3.2)

Generated by `train_bpe.py`.
