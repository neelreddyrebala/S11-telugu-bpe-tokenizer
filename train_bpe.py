#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Train BPE on Telugu (or any Indian language) corpus.
- Reads all .txt files under ./data
- Trains BPE with vocab_size_limit <= 5000
- Evaluates compression ratio on held-out split
- Saves tokenizer to ./artifacts
- Writes token_count and compression ratio into README.md
"""

import os, glob, random, io
from pathlib import Path
from bpe import BPETokenizer, compression_ratio, normalize

DATA_DIR = Path("data")
ARTIFACTS = Path("artifacts")
ARTIFACTS.mkdir(exist_ok=True)

def read_corpus():
    texts = []
    for p in DATA_DIR.glob("**/*.txt"):
        with open(p, "r", encoding="utf-8") as f:
            texts.append(f.read())
    return texts

def split_corpus(texts, val_ratio=0.1, seed=42):
    random.Random(seed).shuffle(texts)
    n_val = max(1, int(len(texts)*val_ratio)) if len(texts) > 1 else 1
    return texts[n_val:], texts[:n_val]

def train_bpe(vocab_limit=5000, min_pair_freq=2):
    texts = read_corpus()
    if not texts:
        raise SystemExit(
            "No training data found under ./data. Please add Telugu (or chosen language) .txt files."
        )
    train_texts, val_texts = split_corpus(texts, val_ratio=0.1)

    # Train
    tok = BPETokenizer()
    tok.train(train_texts, vocab_size_limit=vocab_limit, min_pair_freq=min_pair_freq, progress=True)

    # Evaluate on validation
    val_joined = normalize("\n".join(val_texts))
    ids = tok.encode(val_joined)
    cr = compression_ratio(val_joined, ids)

    # Save artifacts
    prefix = str(ARTIFACTS / "telugu_bpe")
    tok.save(prefix)

    # Stats
    token_count = len(tok.vocab)
    print(f"\n=== Training done ===")
    print(f"Vocab size: {token_count}")
    print(f"Validation compression ratio: {cr:.4f}")

    # Update README.md with stats
    readme_path = Path("README.md")
    if readme_path.exists():
        with open(readme_path, "r", encoding="utf-8") as f:
            content = f.read()
    else:
        content = ""

    marker = "<!-- STATS_MARKER -->"
    stats_block = f"""\
### Results (Auto-filled)
- **Tokenizer vocabulary size**: `{token_count}` (must be < 5000)
- **Compression ratio (val)**: `{cr:.4f}` (must be â‰¥ 3.2)

Generated by `train_bpe.py`.
"""
    if marker in content:
        pre, post = content.split(marker, 1)
        content = pre + marker + "\n\n" + stats_block
    else:
        content += "\n\n" + marker + "\n\n" + stats_block

    with open(readme_path, "w", encoding="utf-8") as f:
        f.write(content)

    # Save plain stats too
    with open(ARTIFACTS / "stats.json", "w", encoding="utf-8") as f:
        import json
        json.dump({"vocab_size": token_count, "compression_ratio": cr}, f, indent=2, ensure_ascii=False)

    return token_count, cr

if __name__ == "__main__":
    train_bpe()
